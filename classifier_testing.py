from sklearn import tree
from sklearn import linear_model
import xgboost as xgb
import sys
import numpy as np

# For the 5v12 seed starting with 2017
# Format: Kenpom, Sagarin
features = [[24.29, 90.21, 12.46, 80.08], [19.74, 87.95, 12.61, 79.53], [16.00, 89.55, 14.95, 81.51], [23.45, 84.89, 13.31, 83.83],
			[20.25, 87.26, 9.57, 78.38], [19.06, 85.79, 14.33, 81.77], [23.85, 89.27, 13.24, 76.00], [23.05, 89.24, 6.50, 80.13],
			[17.64, 86.02, 11.87, 80.45], [16.44, 84.71, 6.72, 77.98], [27.24, 86.37, 13.64, 76.69], [20.98, 90.82, 5.67, 82.36],
			[19.80, 85.81, 10.83, 79.83], [19.24, 86.32, 17.58, 84.88], [15.72, 84.69, 11.60, 80.59], [17.90, 84.29, 11.76, 81.12],
			[18.69, 85.60, 16.78, 85.88], [22.57, 87.56, 16.95, 85.26], [16.07, 87.08, 13.11, 80.11], [20.28, 83.93, 11.52, 81.88],
			#[22.36, 87.37, 13.45, 75.62], [19.45, 80.06, 69.14, 15.27], [20.90, 79.14, 14.34, 74.74], [14.49, 78.48, 13.13, 65.56],
			[20.11, 86.63, 18.16, 84.09], [19.65, 88.35, 6.98, 79.94], [17.33, 84.59, 16.01, 84.88], [17.42, 85.22, 19.32, 85.01],
			[20.12, 86.33, 15.92, 84.86], [20.13, 87.59, 6.03, 77.18], [22.45, 89.57, 16.53, 83.76], [21.33, 87.28, 19.9, 83.33],
			[17.8, 83.79, 15.96, 83.75], [21.84, 88.71, 9.7, 78.73], [15.76, 84.62, 17.62, 84.65], [18.72, 86.25, 8.1, 79.02]
]

# features = [[[24.29, 5], [12.46,12]], [[19.74, 5], [12.61, 12]], [[16.00, 5], [14.95, 12]], [[23.45, 5], [13.31, 12]],
# 			[[20.25, 5], [9.57, 12]], [[19.06, 5], [14.33, 12]], [[23.85, 5], [13.24, 12]], [[23.05, 5], [6.50, 12]],
# 			[[17.64, 5], [11.87, 12]], [[16.44, 5], [6.72, 12]], [[27.24, 5], [13.64, 12]], [[20.98, 5], [5.67, 12]],
# 			[[19.80, 5], [10.83, 12]], [[19.24, 5], [17.58, 12]], [[15.72, 5], [11.60, 12]], [[17.90, 5], [11.76, 12]],
# 			[[18.69, 5], [16.78, 12]], [[22.57, 5], [16.95, 12]], [[16.07, 5], [13.11, 12]], [[20.28, 5], [11.52, 12]]
# ]


labels = [5, 5, 12, 5,
		  5, 12, 12, 5,
		  5, 5, 5, 5,
		  12, 12, 5, 12,
		  12, 12, 12, 5,
		  # 2012 GOES HERE
		  5, 5, 12, 5,
		  12, 5, 5, 5,
		  12, 5, 12, 12
]

# clf = tree.DecisionTreeClassifier()
# clf = linear_model.LogisticRegression()
clf = xgb.XGBClassifier()
clf = clf.fit(np.array(features),np.array(labels))

# print clf.predict(sys.argv[1], sys.argv[2])

# print clf.predict([[float(sys.argv[1]), float(sys.argv[2])]])

# 2018
print clf.predict(np.array([[20.44, 88.22, 14.70, 82.16]])), 5
print clf.predict(np.array([[21.16, 86.49, 10.20, 78.54]])), 5
print clf.predict(np.array([[22.44, 90.26, 12.19, 78.60]])), 5
print clf.predict(np.array([[21.65, 86.70, 12.92, 80.06]])), 5

# 2012
print clf.predict(np.array([[22.36, 87.37, 13.45, 75.62]])), 12
print clf.predict(np.array([[19.45, 80.06, 69.14, 15.27]])), 5
print clf.predict(np.array([[20.90, 79.14, 14.34, 74.74]])), 5
print clf.predict(np.array([[14.49, 78.48, 13.13, 65.56]])), 12
